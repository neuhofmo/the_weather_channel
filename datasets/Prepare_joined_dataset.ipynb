{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare joined datasets for meteorology challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = r'C:\\Users\\User\\Documents\\DMBI_hackathon_2018\\the_weather_channel-master\\the_weather_channel-master\\kaggle_data'\n",
    "os.chdir(working_folder)  # change to your own folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "# train sets\n",
    "Tmax_train = pd.read_csv(r'train_maxTemp.csv')\n",
    "Tmin_train = pd.read_csv(r'train_minTemp.csv')\n",
    "Wind_train = pd.read_csv(r'train_Wind.csv')\n",
    "Humidity_train = pd.read_csv(r'train_Humidity.csv')\n",
    "\n",
    "# test sets\n",
    "Tmax_test = pd.read_csv(r'test_maxTemp.csv')\n",
    "Tmin_test = pd.read_csv(r'test_minTemp.csv')\n",
    "Wind_test = pd.read_csv(r'test_Wind.csv')\n",
    "Humidity_test = pd.read_csv(r'test_Humidity.csv')\n",
    "\n",
    "# obs values\n",
    "train_Tmax_obs = pd.read_csv(r'train_label.csv')\n",
    "test_Tmax_obs = pd.read_csv(r'test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join data sets by keys: Form, Validity date, City\n",
    "# joined_train = pd.merge(Tmax_train, Tmin_train.drop(columns=['Basis date']), on=['Time', 'Validity date', 'City'], suffixes=('_tmax', '_tmin'))\n",
    "\n",
    "\n",
    "joined_train_1 = pd.merge(Tmax_train, Tmin_train, on=['Time', 'Validity date', 'City'], suffixes=('_tmax', '_tmin'))\n",
    "joined_train_2 = pd.merge(joined_train_1, Wind_train, on=['Time', 'Validity date', 'City'], suffixes=('', '_wind'))\n",
    "joined_train = pd.merge(joined_train_2, Humidity_train, on=['Time', 'Validity date', 'City'], suffixes=('', '_humidity'))\n",
    "joined_train.columns = ['Time', 'Basis date_tmax', 'Validity date', 'City',\n",
    "       'Persist. value_tmax', 'EC_tmax', 'CO_tmax', 'C3_tmax', 'OH_tmax',\n",
    "       'Basis date_tmin', 'Persist. value_tmin', 'EC_tmin', 'CO_tmin',\n",
    "       'C3_tmin', 'OH_tmin', 'Basis date_wind', 'Persist. value_wind', 'EC_wind', 'CO_wind', 'C3_wind',\n",
    "       'OH_wind', 'Basis date_humidity', 'Persist. value_humidity', 'EC_humidity',\n",
    "       'C3_humidity']\n",
    "\n",
    "joined_test = pd.merge(Tmax_test, Tmin_test, on=['Time', 'Validity date', 'City'], suffixes=('_tmax', '_tmin'))\n",
    "joined_test = pd.merge(joined_test, Wind_test, on=['Time', 'Validity date', 'City'], suffixes=('', '_wind'))\n",
    "joined_test = pd.merge(joined_test, Humidity_test, on=['Time', 'Validity date', 'City'], suffixes=('', '_humidity'))\n",
    "joined_test.columns = ['Time', 'Basis date_tmax', 'Validity date', 'City',\n",
    "       'Persist. value_tmax', 'EC_tmax', 'CO_tmax', 'C3_tmax', 'OH_tmax',\n",
    "       'Basis date_tmin', 'Persist. value_tmin', 'EC_tmin', 'CO_tmin',\n",
    "       'C3_tmin', 'OH_tmin', 'Basis date_wind', 'Persist. value_wind', 'EC_wind', 'CO_wind', 'C3_wind',\n",
    "       'OH_wind', 'Basis date_humidity', 'Persist. value_humidity', 'EC_humidity',\n",
    "       'C3_humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting string values to float (the model can fit only float values)\n",
    "\n",
    "# casting 'Validity date' to datetime.timestamp\n",
    "def castDates(df):\n",
    "    df['Validity date'] = df['Validity date'].apply(lambda x: (datetime.strptime(x, '%d-%m-%y')).timestamp())\n",
    "\n",
    "\n",
    "def merge12(df):\n",
    "    df1 = df[df['Time']== 1]\n",
    "    df2 = df[df['Time']== 2]\n",
    "    merged_df = pd.merge(df1, df2, on=['Validity date', 'City'], suffixes=('_1', '_2'))\n",
    "    merged_df.drop(['Time_1', 'Time_2'], axis=1, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def drop_basis(df):\n",
    "    return df.drop([x for x in df.columns if x.startswith('Basis date')], axis=1)\n",
    "\n",
    "\n",
    "def prepare_and_process_set(df):\n",
    "    df['City'] = leCity.transform(df['City'])\n",
    "    castDates(df)\n",
    "    joined_12 = merge12(df)\n",
    "    joined_12 = drop_basis(joined_12)\n",
    "    return joined_12\n",
    "    \n",
    "    \n",
    "\n",
    "leCity = preprocessing.LabelEncoder()\n",
    "leCity.fit(joined_train['City'])\n",
    "joined_train_12 = prepare_and_process_set(joined_train)\n",
    "joined_test_12 = prepare_and_process_set(joined_test)\n",
    "# joined_train['City'] = leCity.transform(joined_train['City'])\n",
    "# castDates(joined_train)\n",
    "\n",
    "# joined_test['City'] = leCity.transform(joined_test['City'])\n",
    "# castDates(joined_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_train_12.to_pickle('joined_train_12_df.pickle')\n",
    "joined_test_12.to_pickle('joined_test_12_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_n_days(df, n):\n",
    "    \"\"\"Merge n days prior to each Validation Date in df. Returns df\"\"\"\n",
    "    shifted_list = [df.groupby(['City']).shift(-1*i) for i in range(1, n+1)]\n",
    "    for i in range(n):\n",
    "        back = -1*(i+1)\n",
    "        df = df.join(shifted_list[i].rename(columns=lambda x: x+f\"_{back}\"))\n",
    "        df.drop(f'Validity date_{back}', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df_ts_list = [merge_n_days(joined_train_12, i) for i in range(1, 10)]\n",
    "test_df_ts_list = [merge_n_days(joined_test_12, i) for i in range(1, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a linear Regression model\n",
    "reg = LinearRegression()\n",
    "# reg.fit(joined_train, train_Tmax_obs['observedMaxTemp'])\n",
    "reg.fit(joined_train_12, train_Tmax_obs['observedMaxTemp'])  # only for 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = reg.predict(joined_test_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65650385966\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the model\n",
    "\n",
    "# Calculate rmse by the difference square of the model predicted values and the actual values (obs)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(test_Tmax_obs['observedMaxTemp'], predicted_values))\n",
    "\n",
    "print(rmse)\n",
    "\n",
    "# old: 1.75672391465"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame containing 2 columns:\n",
    "# 1. key -concatenation of station name and date\n",
    "# 2. predicted values\n",
    "\n",
    "# dates = joined_test['Validity date'].apply(lambda x: datetime.fromtimestamp(x).strftime('%d-%m-%y') + '_')\n",
    "# cities = leCity.inverse_transform(joined_test['City'])\n",
    "# prediction_data = pd.DataFrame()\n",
    "# prediction_data['validityDate_city'] = dates + cities\n",
    "# prediction_data['predictedMaxTemp'] = predicted_values\n",
    "\n",
    "# prediction_data.to_csv(r'sampleSubmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
